{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grasp-and-Lift(GAL) EEG Detection\n",
    "## Hand movements の分類\n",
    "- 被験者数: **12**\n",
    "- 各被験者ごとの試行のデータ系列数: **10**\n",
    "- 各被験者のひとつのデータ系列内の試行回数: **約30**(試行回数は各系列データごとに異なる) \n",
    "\n",
    "training set: 各被験者の最初の8つの試行のデータ系列<br/>\n",
    "test set: 第9,10番目の試行のデータ系列<br/>\n",
    "\n",
    "### ラベル\n",
    "各GALには、6つのイベントを検出するタスク \n",
    "(それぞれのイベントにおいて2値分類(ラベル0,1))\n",
    "　\n",
    "1. HandStart\n",
    "1. FirstDigitTouch\n",
    "1. BothStartLoadPhase\n",
    "1. LiftOff\n",
    "1. Replace\n",
    "1. BothReleased\n",
    "\n",
    "これらのイベントは常に同じ順序で発生する<br/>\n",
    "training setには、各件名+シリーズの組み合わせごとに2つのファイル<br/>\n",
    "\n",
    "### データ\n",
    "* *_data.csvファイルには、rawの32チャネルEEG(Electroencephalography, 脳波)データ（サンプリングレート500Hz）\n",
    "* *_events.csvファイルには、すべてのイベントのフレーム・ワイズ・ラベル(1の連続)が含まれる\n",
    " * 6つのラベル列は、対応するイベントが±150ms（±75フレーム）以内に発生したかどうかに応じて、ゼロまたは1のいずれか\n",
    "\n",
    "## 目標\n",
    "#### 理想: イベントの窓全体を完璧に予測\n",
    "\n",
    "## 注意\n",
    "#### 未来データは使用できない(予測する系列の平均などはとれない)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## μ律動\n",
    "7～12Hzのアーチ状の連続した波で，中心・頭頂部に一側性または両側性に出現する．\n",
    "開眼時には減衰しないが，手を握るなどの運動や感覚刺激により抑制される"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/y3_Izuop2gY\" frameborder=\"0\" allowfullscreen></iframe>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "HTML(r'<iframe width=\"560\" height=\"315\" src=\"https://www.youtube.com/embed/y3_Izuop2gY\" frameborder=\"0\" allowfullscreen></iframe>')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# パッケージの準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy\n",
    "import scipy.signal as signal\n",
    "from scipy import fftpack\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from glob import glob\n",
    "import os\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "# LeaveOneGroupOut交差検定\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.model_selection import train_test_split\n",
    "# AUCスコア\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# keras\n",
    "from keras.utils.np_utils import to_categorical\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers.convolutional import Conv2D\n",
    "from keras.layers.pooling import MaxPool2D\n",
    "from keras.optimizers import SGD\n",
    "\n",
    "from keras.layers.core import Dense, Activation, Dropout, Flatten\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.utils import plot_model\n",
    "from keras.callbacks import TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 関数の準備"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### データの読み込み ###\n",
    "\n",
    "def prepare_data_train(fname):\n",
    "    \"\"\" 訓練データの読み込み \"\"\"\n",
    "    # EEGデータ読み込み\n",
    "    data = pd.read_csv(fname)\n",
    "    # fnameイベントファイルの名前に変換\n",
    "    events_fname = fname.replace('_data','_events')\n",
    "    # イベントデータの読み込み\n",
    "    labels= pd.read_csv(events_fname)\n",
    "    clean=data.drop(['id' ], axis=1)#id列を削除\n",
    "    labels=labels.drop(['id' ], axis=1)#id列を削除\n",
    "    return  clean,labels\n",
    "\n",
    "def prepare_data_test(fname):\n",
    "    \"\"\" テストデータの読み込み \"\"\"\n",
    "    # EEGデータの読み込み\n",
    "    data = pd.read_csv(fname)\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "### 前処理 ###\n",
    "\n",
    "def preprocess_median_filter(X, kernel):\n",
    "    \"\"\" Median filter\"\"\"\n",
    "    X_m = signal.medfilt(X, kernel_size=kernel)\n",
    "    return X_m\n",
    "\n",
    "def preprocess_fir_filter(X, fc):\n",
    "    \"\"\" FIR filter \"\"\"\n",
    "    fs = 500\n",
    "    nyq = fs / 2.0  # ナイキスト周波数\n",
    "\n",
    "    # フィルタの設計\n",
    "    # ナイキスト周波数が1になるように正規化\n",
    "    fe = fc / nyq      # カットオフ周波数1\n",
    "    numtaps = 15          # フィルタ係数（タップ）の数（要奇数）\n",
    "\n",
    "    b = scipy.signal.firwin(numtaps, fe) # Low-pass\n",
    "\n",
    "    # FIRフィルタをかける\n",
    "    X_FIR = scipy.signal.lfilter(b, 1, X)\n",
    "    return X_FIR\n",
    "\n",
    "def cut_off(X, fs):\n",
    "    \"\"\" FFT処理後に高周波を取り除く\"\"\"\n",
    "    # fs: カットオフ周波数[Hz]\n",
    "    # 時系列のサンプルデータ作成\n",
    "    n = X.shape[0]                         # データ数\n",
    "    dt = 0.002                       # サンプリング間隔\n",
    "    f = 500                           # 周波数\n",
    "\n",
    "    # FFT 処理と周波数スケールの作成\n",
    "    X_f = fftpack.fft(X)/(n/2)\n",
    "    freq = fftpack.fftfreq(n, dt)\n",
    "\n",
    "    # フィルタ処理\n",
    "    # ここではカットオフ周波数以上に対応するデータを 0 にしている                          \n",
    "    X_f2 = np.copy(X_f)\n",
    "    X_f2[(freq > fs)] = 0\n",
    "    X_f2[(freq < 0)] = 0\n",
    "\n",
    "    # 逆 FFT 処理\n",
    "    # FFT によるフィルタ処理では虚数部が計算されることがあるため\n",
    "    # real 関数が必要(普段は必要ない)\n",
    "    X_prep = np.real(fftpack.ifft(X_f2)*n)\n",
    "    \n",
    "    return X_prep\n",
    "\n",
    "\n",
    "def data_preprocess_train(X):\n",
    "    scaler= StandardScaler()\n",
    "    X_prep = scaler.fit_transform(X)\n",
    "#     X_prep = preprocess_fir_filter(X_prep, 100.0)\n",
    "#     X_prep = cut_off(X, 50.0)\n",
    "\n",
    "    #ここで他のpreprocessingを追加\n",
    "    return X_prep\n",
    "\n",
    "def data_preprocess_test(X):\n",
    "    scaler= StandardScaler()\n",
    "    X_prep = scaler.transform(X)\n",
    "    #ここで他のpreprocessingを追加\n",
    "    return X_prep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "フィルタ処理はどうしたらいいかわからない"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ダウンサンプリングてきななにか\n",
    "subsample=100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 各GALごとに, 6 イベント(ラベル列):\n",
    "\n",
    "1. HandStart\n",
    "1. FirstDigitTouch\n",
    "1. BothStartLoadPhase\n",
    "1. LiftOff\n",
    "1. Replace\n",
    "1. BothReleased"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "cols = ['HandStart','FirstDigitTouch',\n",
    "        'BothStartLoadPhase','LiftOff',\n",
    "        'Replace','BothReleased']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 訓練過程 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 交差検証法を用いたAUC(Area Under the Curve)による予測能の比較\n",
    "- AUC: ROC曲線(Receiver Operatorating Characteristic curve、受信者動作特性曲線)の面積\n",
    "- 混合行列を定量的に比較し，予測能を判断するもの"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DeepNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject 1\n",
      "\n",
      "Fold:  1 \n",
      "\n",
      "Epoch 1/1\n",
      "1302896/1302896 [==============================] - 108s - loss: 0.1536 - acc: 0.9698   \n",
      "acc: 95.73%\n",
      "119264/119496 [============================>.] - ETA: 0s\n",
      "Fold:  2 \n",
      "\n",
      "Epoch 1/1\n",
      "1150438/1150438 [==============================] - 107s - loss: 0.1788 - acc: 0.9634   \n",
      "acc: 98.12%\n",
      "271680/271954 [============================>.] - ETA: 0s\n",
      "Fold:  3 \n",
      "\n",
      "Epoch 1/1\n",
      " 685000/1204778 [================>.............] - ETA: 48s - loss: 0.1911 - acc: 0.9600"
     ]
    }
   ],
   "source": [
    "#######number of subjects###############\n",
    "subjects = range(1,13)\n",
    "series = range(1, 9)\n",
    "pred_tot = []\n",
    "y_tot = []\n",
    "global_auc = []\n",
    "###loop on subjects and 8 series for train data + 2 series for test data\n",
    "for i, subject in enumerate(subjects):\n",
    "    print('Subject %d' % (subject))\n",
    "    y_raw= []\n",
    "    raw = []\n",
    "    sequence = []\n",
    "    auc_tot = []\n",
    "    ################ READ DATA ################################################\n",
    "    for ser in series:\n",
    "        fname =  'input/train/subj%d_series%d_data.csv' % (subject,ser)\n",
    "        data,labels=prepare_data_train(fname)\n",
    "        raw.append(data)\n",
    "        y_raw.append(labels)\n",
    "        sequence.extend([ser]*len(data))\n",
    "\n",
    "    X = pd.concat(raw)\n",
    "    y = pd.concat(y_raw)\n",
    "    #transform in numpy array\n",
    "    #transform train data in numpy array\n",
    "    X = np.asarray(X.astype(float))\n",
    "    y = np.asarray(y.astype(float))\n",
    "    sequence = np.asarray(sequence)\n",
    "#     print(sequence.shape, y.shape, X.shape)\n",
    "    y_binary = to_categorical(y[:, 0])\n",
    "    \n",
    "    \n",
    "    ########### Palameter ######################################################\n",
    "    n_in = len(X[0])\n",
    "    n_hiddens = [200, 200, 200, 200]\n",
    "    n_out = len(y_binary[0])\n",
    "    p_keep = 0.5\n",
    "    activation = 'relu'\n",
    "\n",
    "    # model = Sequential()\n",
    "    # for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "    #     model.add(Dense(n_hiddens[i], input_dim=input_dim))\n",
    "    #     model.add(Activation(activation))\n",
    "    #     model.add(Dropout(p_keep))\n",
    "\n",
    "    # model.add(Dense(n_out))\n",
    "    # model.add(Activation('softmax'))\n",
    "\n",
    "    # model.compile(loss='categorical_crossentropy',\n",
    "    #               optimizer=SGD(lr=0.00001),\n",
    "    #               metrics=['accuracy'])\n",
    "\n",
    "    epochs = 1\n",
    "    batch_size = 200\n",
    "\n",
    "    ################ Train classifiers ########################################\n",
    "    cv = LeaveOneGroupOut()\n",
    "    cv.get_n_splits(groups=sequence)\n",
    "    cvscores = []\n",
    "    # pred = np.empty((X.shape[0],6))\n",
    "    cvfold = 1\n",
    "    auc_tot = []\n",
    "\n",
    "    for train, test in cv.split(X, y_binary, sequence):\n",
    "        print('\\nFold: ', cvfold, '\\n')\n",
    "        cvfold = cvfold + 1\n",
    "        X_train, X_test = X[train], X[test]\n",
    "        y_train, y_test = y_binary[train], y_binary[test]\n",
    "        #apply preprocessing\n",
    "    #     X_train = data_preprocess_train(X_train)\n",
    "    #     X_test = data_preprocess_test(X_test)\n",
    "\n",
    "        model = Sequential()\n",
    "        for i, input_dim in enumerate(([n_in] + n_hiddens)[:-1]):\n",
    "            model.add(Dense(n_hiddens[i], input_dim=input_dim))\n",
    "            model.add(BatchNormalization())\n",
    "            model.add(Activation(activation))\n",
    "            model.add(Dropout(p_keep))\n",
    "\n",
    "        model.add(Dense(n_out))\n",
    "        model.add(Activation('softmax'))\n",
    "\n",
    "        model.compile(loss='categorical_crossentropy',\n",
    "                      optimizer=SGD(lr=0.001),\n",
    "                      metrics=['accuracy'])\n",
    "        model.fit(X_train, y_train, epochs=epochs,\n",
    "                     batch_size=batch_size, verbose=1)\n",
    "\n",
    "        scores = model.evaluate(X_test, y_test, verbose=0)\n",
    "        print(\"%s: %.2f%%\" % (model.metrics_names[1], scores[1]*100))\n",
    "        cvscores.append(scores[1] * 100)\n",
    "        pred = model.predict_proba(X_test)\n",
    "        auc = roc_auc_score(y_test,pred)    \n",
    "        auc_tot.append(auc)\n",
    "        print(auc, '\\n')\n",
    "\n",
    "    auc_tot = np.asarray(auc_tot)\n",
    "    print(auc_tot)\n",
    "    print('Mean AUC: ', np.mean(auc_tot))\n",
    "    global_auc.append(np.mean(auc_tot))\n",
    "\n",
    "        \n",
    "#     preds = Parallel(n_jobs=6)(delayed(predict)(clfs[i],X_test) for i in range(6))\n",
    "#     pred[test,:] = np.concatenate(preds,axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "global_auc\n",
    "sum(global_auc) / float(len(global_auc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'np' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-37b6d5b3fdf9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mauc_tot\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mauc_tot\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mauc_tot\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcolumns\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcols\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubjects\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'results_cv_auc.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'np' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(1, len(global_acc)+1)\n",
    "plt.bar(x, global_auc)\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('CV auc for each subject')\n",
    "plt.savefig('cross_val_auc_subject.png' ,bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96.86% (+/- 1.18%)\n"
     ]
    }
   ],
   "source": [
    "print(\"%.2f%% (+/- %.2f%%)\" % (np.mean(cvscores), np.std(cvscores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "x = range(1, len(cvscores)+1)\n",
    "plt.bar(x, cvscores)\n",
    "plt.xlabel('Subject')\n",
    "plt.ylabel('AUC')\n",
    "plt.title('CV auc for each subject')\n",
    "plt.savefig('cross_val_auc_subject.png' ,bbox_inches='tight')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
